# -*- coding: utf-8 -*-
"""Latest Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x6FxqdRpC-S5gumppflgdvgd5-uTasok
"""

# !pip install wandb -qU

import wandb
import os
import numpy as np
from keras.datasets import fashion_mnist
from keras.datasets import mnist
from sklearn.model_selection import train_test_split
import argparse

os.environ['WAND_NOTEBOOK_NAME'] = 'Latest Final'

# !wandb login c2d81ac4a00568bc9c2f420ec019b35bd24aac61

# To input values as command line arguments I used argparse

parser = argparse.ArgumentParser()

parser.add_argument("-wp","--wandb_project",help="projectname")
parser.add_argument("-we","--wandb_entity",help="entityname")
parser.add_argument("-d","--dataset",help="datasetname",choices=['mnist', 'fashion_mnist'])
parser.add_argument("-e","--epochs",help="epochs",choices=['5','10'])
parser.add_argument("-b","--batch_size",help="mini_batch_size",choices=['16','32','64'])
parser.add_argument("-l","--loss",help="loss_functions",choices=['cross_entropy','mean_squared_error'])
parser.add_argument("-o","--optimizer",help="optimizer",choices=['sgd','momentum','nag','rmsprop','adam','nadam'])
parser.add_argument("-lr","--learning_rate",help="learning_rates",choices=['1e-3','1e-4'])
parser.add_argument("-m","--momentum",help="momentum_nag_and_rmsprop_beta",choices=['0.5'])
parser.add_argument("-beta","--beta",help="beta",choices=['0.5'])
parser.add_argument("-beta1","--beta1",help="beta1",choices=['0.5'])
parser.add_argument("-beta2","--beta2",help="beta2",choices=['0.5'])
parser.add_argument("-eps","--epsilon",help="epsilon",choices=['0.000001'])
parser.add_argument("-w_d","--weight_decay",help="weight_decays",choices=['0','0.0005','0.5'])
parser.add_argument("-w_i","--weight_init",help="weight_init",choices=['random','Xavier'])
parser.add_argument("-nhl","--num_layers",help="number_of_layers",choices=['3','4','5'])
parser.add_argument("-sz","--hidden_size",help="hidden_layer_size",choices=['32','64','128'])
parser.add_argument("-a","--activation",help="activation",choices=['sigmoid','tanh','ReLU'])

args = parser.parse_args()

proj_name = 'CS6910_A1'
entity_name = 'cs22m064'
data_set = 'fashion_mnist'
ep = 10
bs = 32
lf = 'cross_entropy'
op = 'nadam'
lr = 1e-3
m_beta = 0.9
rmsprop_beta = 0.9
beta_1 = 0.9
beta_2 = 0.999
epsilon_ = 1e-3
wdc = 0
winit = 'Xavier'
nhls = 3
hls = 128
af = 'ReLU'

# Check if entered argument is none otherwise assigning it to our variables

if args.wandb_project!=None:
  proj_name = args.wandb_project
if args.wandb_entity!=None:
  entity_name = args.wandb_entity
if args.dataset!=None:
  data_set = args.dataset 
if args.epochs!=None:
  ep = float(args.epochs)
if args.batch_size!=None:
  bs = int(args.batch_size)
if args.loss!=None:
  lf = args.loss
if args.optimizer!=None:
  op = args.optimizer
if args.learning_rate!=None:
  lr = float(args.learning_rate)
if args.momentum!=None:
  m_beta = float(args.momentum)
if args.beta!=None:
  rmsprop_beta = float(args.beta)
if args.beta1!=None:
  beta_1 = float(args.beta1)
if args.beta2!=None:
  beta_2 = float(args.beta2)
if args.epsilon!=None:
  epsilon_ = float(args.epsilon)
if args.weight_decay!=None:
  wdc = float(args.weight_decay)
if args.weight_init !=None:
  winit = args.weight_init
if args.num_layers != None:
  nhls = int(args.num_layers)
if args.hidden_size!=None:
  hls = int(args.hidden_size)
if args.activation!=None:
  af =  args.activation

classes=['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']

# code to run sweeps in wandb

# search method is bayes here
sweep_config ={
    'method':'bayes'
}

# here we want W&B sweeps to maximize our validation accuracy
metric = {
    'name' : 'validation_accuracy',
    'goal' : 'maximize'
}
sweep_config['metric'] = metric

# various values of hyperparameters are specified here that are used for finding best model
parameters_dict={
    'nhl':{
        'values' : [3,4,5]
    },
    'hls':{
        'values' : [32,64,128]
    },
    'eta':{
        'values' : [1e-3,1e-4]
    },
    'wi':{
        'values' : ['random','Xavier']
    },
    'af':{
        'values' : ['sigmoid','tanh','ReLU']
    },
    'epochs':{
        'values' : [5,10]
    },
    'bs':{
        'values' : [16,32,64]
    },
    'opt':{
        'values' : ['sgd','momentum','nag','rmsprop','adam','nadam']
    },
    'wdc':{
        'values' : [0,0.0005,0.5]
    }
}
sweep_config['parameters'] = parameters_dict

# generating sweep id
sweep_id = wandb.sweep(sweep_config,project=proj_name)

# creating one hot vectors for labels
def map_data_with_classes(classes):

  maxi = 0
  i = 0
  while(i!=len(classes)):
    if(maxi<classes[i]) :
      maxi = classes[i]
    i+=1
  
  cols = maxi + 1
  rows = len(classes)
  matrix = np.zeros((rows,cols))

  j = 0
  while(j!=len(classes)):
    matrix[j][classes[j]] = 1
    j+=1
  return matrix

# loading and preprocessing data
if data_set == 'fashion_mnist':
  (train_X,train_Y),(test_X,test_Y) = fashion_mnist.load_data()
elif data_set == 'mnist':
  (train_X,train_Y),(test_X,test_Y) = mnist.load_data()

train_X = train_X/255
test_X = test_X/255

needed_y_train = train_Y
needed_y_test = test_Y

# splitting data into train and validation sets
trainX, val_X, trainy, valy = train_test_split(train_X, train_Y, test_size=0.1, random_state=40)

#flatten 2d image vectors to 1d vectors and treat them as training data
trainX = trainX.reshape(len(trainX),len(trainX[0])*len(trainX[1]))
testX = test_X.reshape(len(test_X),len(test_X[0])*len(test_X[1]))
valX = val_X.reshape(len(val_X),len(val_X[0])*len(val_X[1]))

trainy = map_data_with_classes(trainy)
testy = map_data_with_classes(test_Y)
valiy = map_data_with_classes(valy)

input_layer_size = len(trainX[0])
output_layer_size = len(trainy[0])

# function to initialize the weights and biases
def initialize_weights_and_biases(layers,number_hidden_layers = 1,init_type='random'):
  weights = []
  biases = []
  if(init_type == 'random'):
    i = 0
    while(i!=number_hidden_layers+1):
      ws = np.random.normal(0,0.5,(layers[i]['output_size'],layers[i]['input_size']))
      weights.append(ws)

      bs = np.random.normal(0,0.5,(layers[i]['output_size'],1))
      biases.append(bs)
      
      i+=1
  elif(init_type == 'Xavier'):
    i = 0
    while(i!=number_hidden_layers+1):
      sum = layers[i]['output_size'] + layers[i]['input_size']
      right_x = np.sqrt(6/(sum))
      left_x = -1 * right_x
      ws = np.random.uniform(left_x, right_x, size=(layers[i]['output_size'], layers[i]['input_size']))
      weights.append(ws)

      bs = np.random.uniform(left_x, right_x, size=(layers[i]['output_size'], 1))
      biases.append(bs)

      i+=1

  return weights,biases

# calculate different activation functions
def sigmoid(x):
  return 1.0/(1.0 + np.exp(-x))

def tanh(x):
  return np.tanh(x)

def relu(x):
  return np.maximum(0,x)

# calculate output funtion softmax
def softmax(x):
  y = []
  i=0
  while(i!=len(x)):
    s = 0
    x[i] -= x[i][np.argmax(x[i])]
    j=0
    while(j!=len(x[i])):
      s+=(np.exp(x[i][j]))
      j+=1
    res = np.exp(x[i])/s
    y.append(res)
    i+=1
  return np.array(y)

# calculate loss as cross entropy
def cross_entropy(y_hat,y):
  epsilon_ce = 1e-9
  error = -(np.multiply(y,np.log(y_hat+epsilon_ce))).sum()/len(y_hat)
  return error

# calculate different activation functions
def activation_functions(x,activation_function) :
  if activation_function == 'sigmoid' :
    return sigmoid(x)
  elif activation_function == 'softmax':
    return softmax(x)
  elif activation_function == 'tanh':
    return tanh(x)
  elif activation_function == 'ReLU':
    return relu(x)
  else:
    return 'error'

# calculate loss as mean squared error
def mean_squared_error(y_hat,y):
  error = np.sum(((y-y_hat)**2)/(2*len(y)))
  return error

# calculate different activation function derivatives
def activation_derivative(x, activation_function):
    if activation_function == "sigmoid":
      return sigmoid(x)*(1.0-sigmoid(x))
    elif activation_function == "tanh":
      return 1.0-(tanh(x)**2)
    elif activation_function == "ReLU":
      return 1. * (x>0)
    else:
      return 'error'

# calculate training accuracy
def train_accuracy(batch_testy,y_predicted,trainy):
  tot = 0
  j = 0
  c = 0
  while(j!=len(batch_testy)):
    k=0
    while(k!=len(batch_testy[j])):
      tot += 1
      l = 0
      while(l!=len(batch_testy[j][k])): 
        if batch_testy[j][k][l] == 1 :
          index_of_one = l
          break
        l+=1
      l = 0
      maxi = 0
      while(l!=len(y_predicted[j][k])): 
        if y_predicted[j][k][l] > maxi :
          maxi = y_predicted[j][k][l]
          pred_class = l
        l+=1
      if(pred_class == index_of_one):
        c+=1
      k+=1
    j+=1
  return (c/len(trainy))

# calculate validation and test accuracy
def test_accuracy(testX,testy,weights,biases,number_hidden_layers,activation_function,output_function):
  a,h = forward_propagation(testX,weights,biases,number_hidden_layers, activation_function, output_function)
  y_pred = h[-1]
  y_predicted = []
  i = 0
  while(i!=len(y_pred)):
    y_predicted.append(np.argmax(y_pred[i]))
    i+=1
  i = 0
  c = 0
  while(i!=len(testy)):
    if y_predicted[i] == testy[i]:
      c+=1
    i+=1
  return c/len(testy)

# calculating regularizing term that is to be added to loss
def calculate_regularizing_term(y,weight_decay_const,number_hidden_layers,weights):
  i = 0
  reg_term = 0.0
  while(i!=number_hidden_layers+1):
    reg_term += (np.sum(weights[i]**2))
    i+=1
  
  reg_term = ((weight_decay_const/(2*len(y)))*(reg_term))
  return reg_term

# calculating validation loss
def val_loss(valX,valy,weights,biases,number_hidden_layers, activation_function,output_function,loss_function):  
  a,h = forward_propagation(valX,weights,biases,number_hidden_layers, activation_function, output_function)
  y_hat = h[-1]
  if loss_function == 'cross_entropy':
    error = cross_entropy(y_hat,valy)
  elif loss_function == 'mean_squared_error':
    error = mean_squared_error(y_hat,valy)
  return error

# forward propagation function
def forward_propagation(batchtrainX,weights,biases,number_hidden_layers,activation_function,output_function):
  a = []
  h = []
  N = len(trainX)

  #initializing a1 with the input
  batch_trainX = np.reshape(batchtrainX,(len(batchtrainX),len(batchtrainX[0])))
  a1 = np.dot(weights[0],batch_trainX.T) + biases[0]
  a.append(a1)
  h1 = activation_functions(a1,activation_function)
  h.append(h1)

  #finding a2 to aL-1 using h1 to hL-2 and h2 to hL-1 using a2 to aL-1
  i = 1
  while(i!=number_hidden_layers):
    an = np.dot(weights[i],h[i-1]) + biases[i]
    a.append(an)
    hn = activation_functions(an,activation_function)
    h.append(hn)
    i+=1
  
  #finding aL and hL as output function differs for this
  aL = np.matmul(weights[number_hidden_layers],h[number_hidden_layers-1]) + biases[number_hidden_layers]
  hL = activation_functions(aL.T,output_function)
  hL = hL.T
  a.append(aL)
  h.append(hL)

  i = 0
  while(i!=number_hidden_layers+1):
    a[i] = a[i].T
    h[i] = h[i].T
    i+=1

  return a,h

# backward propagation function
def backward_propagation(batch_trainy , batch_trainX ,y_hat , a, h, weights, number_hidden_layers ,derivative_function = 'sigmoid'):
  del_a = {}
  del_W = {}
  del_b = {}
  del_h = {}

  batch_trainy = batch_trainy.reshape(len(batch_trainy),len(batch_trainy[0]))
  
  ep =1e-8
  del_a['a'+ str(number_hidden_layers+1)] = -(batch_trainy-y_hat)
  del_h['h'+ str(number_hidden_layers+1)] = -(batch_trainy/(y_hat+ep))

  # starting from last layer and going to first layer
  i = number_hidden_layers + 1
  while(i!=1):
    # calculating del of weights from del of a and h from foward propagation
    del_W['W'+ str(i)] = np.dot(del_a['a' + str(i)].T,h[i-2])

    # applying L2 regularization
    del_W['W'+ str(i)] += (wdc * weights[i-1])
    del_W['W'+ str(i)]/=len(batch_trainX)
    
    # calculating del of biases from del of a
    del_b['b'+ str(i)] = del_a['a'+ str(i)]
    
    # calculating del of h from weights and del of a
    del_h['h'+ str(i-1)] = np.dot(weights[i-1].T , del_a['a' + str(i)].T)

    # calculating del of a from del of h
    del_a['a'+str(i-1)] = np.multiply(del_h['h'+str(i-1)],activation_derivative(a[i-2].T,derivative_function))
    del_a['a'+str(i-1)] = del_a['a'+str(i-1)].T

    i-=1
  
  # for first we only need to calculate del W and del b and not del h and del a
  del_W['W'+str('1')] = np.dot(del_a['a1'].T,batch_trainX)
  del_b['b'+str('1')] = del_a['a1']
  
  j = 1
  while(j!=len(del_b)+1):
    k = 0
    l = 0
    li = []
    for k in range(len(del_b['b'+str(j)][0])) :
      sum = 0
      for l in range(len(del_b['b'+str(j)])) :
          sum += del_b['b'+str(j)][l][k]
      li.append(sum/len(batch_trainX))
    li = np.array(li)
    del_b['b'+str(j)] = li.reshape(len(li),1)
    j+=1

  return del_W,del_b

# gradient decent
def gradient_descent(trainX, trainy, number_hidden_layers = 1, hidden_layer_size = 4, eta = 0.1, initial_weights = 'random', activation_function = 'sigmoid', epochs = 1, output_function = 'softmax', mini_batch_size=4,loss_function = 'cross_entropy',weight_decay_const=0,wandb_flag=False):

#initialize layers of neural networks
  layers = []
  layer1 = {'input_size' : input_layer_size, 'output_size' : hidden_layer_size, 'function' : activation_function}
  layers.append(layer1)
  
  i=0
  while(i!=number_hidden_layers-1):
    hlayer = {'input_size' : hidden_layer_size, 'output_size' : hidden_layer_size, 'function' : activation_function}
    layers.append(hlayer)
    i+=1
  
  layern = {'input_size' : hidden_layer_size, 'output_size' : output_layer_size, 'function' : output_function}
  layers.append(layern)

#initialize weights and biases
  weights,biases =initialize_weights_and_biases(layers,number_hidden_layers,initial_weights)

  number_batches = len(trainX)/mini_batch_size

  mini_batch_trainX = np.array(np.array_split(trainX, number_batches))
  mini_batch_trainy = np.array(np.array_split(trainy, number_batches))

  train_loss_list = []
  val_loss_list = []
  train_acc_list = []
  val_acc_list = []
  h=None
  j = 0
  while(j!=epochs):
    k=0
    tloss = 0
    vloss = 0
    y_predicted = []
    while(k!=len(mini_batch_trainX)):
      a,h = forward_propagation(mini_batch_trainX[k],weights,biases,number_hidden_layers, activation_function, output_function)
      y_predicted.append(h[-1])

      if loss_function == 'cross_entropy':
        tloss += cross_entropy(h[-1],mini_batch_trainy[k])
      elif loss_function == 'mean_squared_error':
        tloss += mean_squared_error(h[-1],mini_batch_trainy[k])
      else:
        print('wrong loss function')
      
      del_W,del_b = backward_propagation(mini_batch_trainy[k],mini_batch_trainX[k],h[-1],a,h,weights,number_hidden_layers ,activation_function)

      i = 0
      while(i!=len(weights)):
        weights[i] = weights[i] - (del_W['W'+str(i+1)]*eta)
        biases[i] = biases[i] - (del_b['b'+str(i+1)]*eta) 
        i+=1
      k+=1
    train_acc = train_accuracy(mini_batch_trainy,y_predicted,trainy)
    reg_term_train = calculate_regularizing_term(trainy,weight_decay_const,number_hidden_layers,weights)
    tr_loss = tloss/number_batches + reg_term_train
    val_acc = test_accuracy(valX,valy,weights,biases,number_hidden_layers,activation_function,output_function)
    vloss = val_loss(valX,valiy,weights,biases,number_hidden_layers,activation_function,output_function,loss_function)
    reg_term_val = calculate_regularizing_term(valiy,weight_decay_const,number_hidden_layers,weights)
    vloss = vloss + reg_term_val
    print("epoch : ",j+1," validation loss : ",vloss)

    train_loss_list.append(tr_loss)
    val_loss_list.append(vloss)
    train_acc_list.append(train_acc)
    val_acc_list.append(val_acc)

    if wandb_flag == True:
      wandb.log({"train_loss":tr_loss,"validation_loss":vloss,"train_accuracy":train_acc,"validation_accuracy":val_acc})
    j+=1
  plot_lists = [train_loss_list,val_loss_list,train_acc_list,val_acc_list]
  return h[-1],weights,biases,plot_lists

def momentum_based_gradient_descent(trainX, trainy, number_hidden_layers = 1, hidden_layer_size = 4, eta = 0.1, initial_weights = 'random', activation_function = 'sigmoid', epochs = 1, output_function = 'softmax', mini_batch_size=4,loss_function = 'cross_entropy',weight_decay_const=0,wandb_flag=False):
  
  

  #initialize layers of neural networks

  layers = []
  layer1 = {'input_size' : input_layer_size, 'output_size' : hidden_layer_size, 'function' : activation_function}
  layers.append(layer1)
  
  i=0
  while(i!=number_hidden_layers-1):
    hlayer = {'input_size' : hidden_layer_size, 'output_size' : hidden_layer_size, 'function' : activation_function}
    layers.append(hlayer)
    i+=1
  
  layern = {'input_size' : hidden_layer_size, 'output_size' : output_layer_size, 'function' : output_function}
  layers.append(layern)

  #initialize weights and biases
  
  number_batches = len(trainX)/mini_batch_size

  weights,biases = initialize_weights_and_biases(layers,number_hidden_layers,initial_weights)

  mini_batch_trainX = np.array(np.array_split(trainX, number_batches))
  mini_batch_trainy = np.array(np.array_split(trainy, number_batches))

  past_weights = []
  past_biases = []

  i = 0
  while(i!=number_hidden_layers+1):
    past_weights.append(np.zeros((len(weights[i]),len(weights[i][0]))))
    past_biases.append(np.zeros((len(biases[i]),len(biases[i][0]))))
    i+=1

  train_loss_list = []
  val_loss_list = []
  train_acc_list = []
  val_acc_list = []
  h=None
  j = 0
  while(j!=epochs):
    k=0
    tloss = 0
    vloss = 0
    y_predicted = []
    while(k!=len(mini_batch_trainX)):
      a,h = forward_propagation(mini_batch_trainX[k],weights,biases,number_hidden_layers, activation_function, output_function)
      y_predicted.append(h[-1])

      if loss_function == 'cross_entropy':
        tloss += cross_entropy(h[-1],mini_batch_trainy[k])
      elif loss_function == 'mean_squared_error':
        tloss += mean_squared_error(h[-1],mini_batch_trainy[k])
      else:
        print('wrong loss function')
        
      del_W,del_b = backward_propagation(mini_batch_trainy[k],mini_batch_trainX[k],h[-1],a,h,weights,number_hidden_layers ,activation_function)

      i = 0
      while(i!=number_hidden_layers+1):
        past_weights[i] = (past_weights[i]*m_beta) + (del_W['W' + str(i+1)] * eta)
        past_biases[i] = (past_biases[i]*m_beta) + (del_b['b' + str(i+1)] * eta)

        weights[i] = weights[i]-past_weights[i]
        biases[i] = biases[i]-past_biases[i]

        i+=1

      k+=1
    train_acc = train_accuracy(mini_batch_trainy,y_predicted,trainy)
    reg_term_train = calculate_regularizing_term(trainy,weight_decay_const,number_hidden_layers,weights)
    tr_loss = tloss/number_batches + reg_term_train
    val_acc = test_accuracy(valX,valy,weights,biases,number_hidden_layers,activation_function,output_function)
    vloss = val_loss(valX,valiy,weights,biases,number_hidden_layers,activation_function,output_function,loss_function)
    reg_term_val = calculate_regularizing_term(valiy,weight_decay_const,number_hidden_layers,weights)
    vloss = vloss + reg_term_val
    print("epoch : ",j+1," validation loss : ",vloss)

    train_loss_list.append(tr_loss)
    val_loss_list.append(vloss)
    train_acc_list.append(train_acc)
    val_acc_list.append(val_acc)

    if wandb_flag == True:
      wandb.log({"train_loss":tr_loss,"validation_loss":vloss,"train_accuracy":train_acc,"validation_accuracy":val_acc})

    j+=1 
  plot_lists = [train_loss_list,val_loss_list,train_acc_list,val_acc_list]
  return h[-1],weights,biases,plot_lists

def nestrov_accelerated_gradient_descent(trainX, trainy, number_hidden_layers = 1, hidden_layer_size = 4, eta = 0.1, initial_weights = 'random', activation_function = 'sigmoid', epochs = 1, output_function = 'softmax', mini_batch_size=4,loss_function = 'cross_entropy',weight_decay_const=0,wandb_flag=False):
  
  #initialize layers of neural networks

  layers = []
  layer1 = {'input_size' : input_layer_size, 'output_size' : hidden_layer_size, 'function' : activation_function}
  layers.append(layer1)
  
  i=0
  while(i!=number_hidden_layers-1):
    hlayer = {'input_size' : hidden_layer_size, 'output_size' : hidden_layer_size, 'function' : activation_function}
    layers.append(hlayer)
    i+=1
  
  layern = {'input_size' : hidden_layer_size, 'output_size' : output_layer_size, 'function' : output_function}
  layers.append(layern)

  #initialize weights and biases

  weights,biases = initialize_weights_and_biases(layers,number_hidden_layers,initial_weights)

  number_batches = len(trainX)/mini_batch_size

  mini_batch_trainX = np.array(np.array_split(trainX, number_batches))
  mini_batch_trainy = np.array(np.array_split(trainy, number_batches))

  past_weights = []
  past_biases = []

  # beta = 0.9

  i = 0
  while(i!=number_hidden_layers+1):
    past_weights.append(np.zeros((len(weights[i]),len(weights[i][0]))))
    past_biases.append(np.zeros((len(biases[i]),len(biases[i][0]))))
    i+=1

  train_loss_list = []
  val_loss_list = []
  train_acc_list = []
  val_acc_list = []
  h=None
  j = 0
  while(j!=epochs):
    k=0
    tloss = 0
    vloss = 0
    y_predicted = []
    while(k!=len(mini_batch_trainX)):
      l=0
      lookahead_weights = []
      lookahead_biases = []
      while(l!=number_hidden_layers+1):
        lookahead_weights.append(weights[l] - (past_weights[l] * m_beta))
        lookahead_biases.append(biases[l] - (past_biases[l] * m_beta))
        l+=1
      a,h = forward_propagation(mini_batch_trainX[k],lookahead_weights,lookahead_biases,number_hidden_layers, activation_function, output_function)
      y_predicted.append(h[-1])

      if loss_function == 'cross_entropy':
        tloss += cross_entropy(h[-1],mini_batch_trainy[k])
      elif loss_function == 'mean_squared_error':
        tloss += mean_squared_error(h[-1],mini_batch_trainy[k])
      else:
        print('wrong loss function')
      
      del_W,del_b = backward_propagation(mini_batch_trainy[k],mini_batch_trainX[k],h[-1],a,h,lookahead_weights,number_hidden_layers ,activation_function)

      i = 0
      while(i!=number_hidden_layers+1):
        past_weights[i] = (past_weights[i]*m_beta) + (del_W['W' + str(i+1)] * eta)
        past_biases[i] = (past_biases[i]*m_beta) + (del_b['b' + str(i+1)] * eta)

        weights[i] = weights[i]-past_weights[i]
        biases[i] = biases[i]-past_biases[i]

        i+=1

      k+=1
    train_acc = train_accuracy(mini_batch_trainy,y_predicted,trainy)
    reg_term_train = calculate_regularizing_term(trainy,weight_decay_const,number_hidden_layers,weights)
    tr_loss = tloss/number_batches + reg_term_train
    val_acc = test_accuracy(valX,valy,weights,biases,number_hidden_layers,activation_function,output_function)
    vloss = val_loss(valX,valiy,weights,biases,number_hidden_layers,activation_function,output_function,loss_function)
    reg_term_val = calculate_regularizing_term(valiy,weight_decay_const,number_hidden_layers,weights)
    vloss = vloss + reg_term_val
    print("epoch : ",j+1," validation loss : ",vloss)

    train_loss_list.append(tr_loss)
    val_loss_list.append(vloss)
    train_acc_list.append(train_acc)
    val_acc_list.append(val_acc)
    
    if wandb_flag == True:
      wandb.log({"train_loss":tr_loss,"validation_loss":vloss,"train_accuracy":train_acc,"validation_accuracy":val_acc})

    j+=1 

  plot_lists = [train_loss_list,val_loss_list,train_acc_list,val_acc_list]
  return h[-1],weights,biases,plot_lists

def rmsprop(trainX, trainy, number_hidden_layers = 1, hidden_layer_size = 4, eta = 0.1, initial_weights = 'random', activation_function = 'sigmoid', epochs = 1, output_function = 'softmax', mini_batch_size=4,loss_function = 'cross_entropy',weight_decay_const=0,wandb_flag=False):

  #initialize layers of neural networks

  layers = []
  layer1 = {'input_size' : input_layer_size, 'output_size' : hidden_layer_size, 'function' : activation_function}
  layers.append(layer1)
  
  i=0
  while(i!=number_hidden_layers-1):
    hlayer = {'input_size' : hidden_layer_size, 'output_size' : hidden_layer_size, 'function' : activation_function}
    layers.append(hlayer)
    i+=1
  
  layern = {'input_size' : hidden_layer_size, 'output_size' : output_layer_size, 'function' : output_function}
  layers.append(layern)

  #initialize weights and biases

  weights,biases = initialize_weights_and_biases(layers,number_hidden_layers,initial_weights)

  number_batches = len(trainX)/mini_batch_size

  mini_batch_trainX = np.array(np.array_split(trainX, number_batches))
  mini_batch_trainy = np.array(np.array_split(trainy, number_batches))

  v_weights = []
  v_biases = []

  # beta = 0.9
  # ep = 1e-3

  i = 0
  while(i!=number_hidden_layers+1):
    v_weights.append(np.zeros((len(weights[i]),len(weights[i][0]))))
    v_biases.append(np.zeros((len(biases[i]),len(biases[i][0]))))
    i+=1

  train_loss_list = []
  val_loss_list = []
  train_acc_list = []
  val_acc_list = []
  h=None
  j = 0
  while(j!=epochs):
    k=0
    tloss = 0
    vloss = 0
    y_predicted = []
    while(k!=len(mini_batch_trainX)):
      l=0
      a,h = forward_propagation(mini_batch_trainX[k],weights,biases,number_hidden_layers, activation_function, output_function)
      y_predicted.append(h[-1])

      if loss_function == 'cross_entropy':
        tloss += cross_entropy(h[-1],mini_batch_trainy[k])
      elif loss_function == 'mean_squared_error':
        tloss += mean_squared_error(h[-1],mini_batch_trainy[k])
      else:
        print('wrong loss function')
      
      del_W,del_b = backward_propagation(mini_batch_trainy[k],mini_batch_trainX[k],h[-1],a,h,weights,number_hidden_layers ,activation_function)

      i = 0
      while(i!=number_hidden_layers+1):
        v_weights[i] = (v_weights[i]*rmsprop_beta) + ((del_W['W' + str(i+1)]**2) * (1-rmsprop_beta))
        v_biases[i] = (v_biases[i]*rmsprop_beta) + ((del_b['b' + str(i+1)]**2) * (1-rmsprop_beta))

        weights[i] = weights[i] - (((del_W['W' + str(i+1)]/np.sqrt(v_weights[i] + epsilon_)))*eta)
        biases[i] = biases[i] - (((del_b['b' + str(i+1)]/np.sqrt(v_biases[i] + epsilon_)))*eta)

        i+=1

      k+=1
    train_acc = train_accuracy(mini_batch_trainy,y_predicted,trainy)
    reg_term_train = calculate_regularizing_term(trainy,weight_decay_const,number_hidden_layers,weights)
    tr_loss = tloss/number_batches + reg_term_train
    val_acc = test_accuracy(valX,valy,weights,biases,number_hidden_layers,activation_function,output_function)
    vloss = val_loss(valX,valiy,weights,biases,number_hidden_layers,activation_function,output_function,loss_function)
    reg_term_val = calculate_regularizing_term(valiy,weight_decay_const,number_hidden_layers,weights)
    vloss = vloss + reg_term_val
    print("epoch : ",j+1," validation loss : ",vloss)

    train_loss_list.append(tr_loss)
    val_loss_list.append(vloss)
    train_acc_list.append(train_acc)
    val_acc_list.append(val_acc)

    if wandb_flag == True:
      wandb.log({"train_loss":tr_loss,"validation_loss":vloss,"train_accuracy":train_acc,"validation_accuracy":val_acc})

    j+=1 
  plot_lists = [train_loss_list,val_loss_list,train_acc_list,val_acc_list]
  return h[-1],weights,biases,plot_lists

def adam(trainX, trainy, number_hidden_layers = 1, hidden_layer_size = 4, eta = 0.1, initial_weights = 'random', activation_function = 'sigmoid', epochs = 1, output_function = 'softmax', mini_batch_size=4,loss_function = 'cross_entropy',weight_decay_const=0,wandb_flag=False):

#initialize layers of neural networks
  layers = []
  layer1 = {'input_size' : input_layer_size, 'output_size' : hidden_layer_size, 'function' : activation_function}
  layers.append(layer1)
  
  i=0
  while(i!=number_hidden_layers-1):
    hlayer = {'input_size' : hidden_layer_size, 'output_size' : hidden_layer_size, 'function' : activation_function}
    layers.append(hlayer)
    i+=1
  
  layern = {'input_size' : hidden_layer_size, 'output_size' : output_layer_size, 'function' : output_function}
  layers.append(layern)

#initialize weights and biases

  weights,biases =initialize_weights_and_biases(layers,number_hidden_layers,initial_weights)

  number_batches = len(trainX)/mini_batch_size

  mini_batch_trainX = np.array(np.array_split(trainX, number_batches))
  mini_batch_trainy = np.array(np.array_split(trainy, number_batches))

  # beta1 = 0.9
  # beta2 = 0.999
  # ep = 1e-3

  v_weights,v_biases,v_hat_weights,v_hat_biases,m_weights,m_biases,m_hat_weights,m_hat_biases = [],[],[],[],[],[],[],[]

  i = 0
  while(i!=number_hidden_layers+1):
    v_weights.append(np.zeros((len(weights[i]),len(weights[i][0]))))
    v_biases.append(np.zeros((len(biases[i]),len(biases[i][0]))))
    v_hat_weights.append(np.zeros((len(weights[i]),len(weights[i][0]))))
    v_hat_biases.append(np.zeros((len(biases[i]),len(biases[i][0]))))
    m_weights.append(np.zeros((len(weights[i]),len(weights[i][0]))))
    m_biases.append(np.zeros((len(biases[i]),len(biases[i][0]))))
    m_hat_weights.append(np.zeros((len(weights[i]),len(weights[i][0]))))
    m_hat_biases.append(np.zeros((len(biases[i]),len(biases[i][0]))))
    i+=1

  train_loss_list = []
  val_loss_list = []
  train_acc_list = []
  val_acc_list = []
  h=None
  j = 0
  c = 0
  while(j!=epochs):
    k=0
    tloss = 0
    vloss = 0
    y_predicted = []
    while(k!=len(mini_batch_trainX)):
      c+=1
      a,h = forward_propagation(mini_batch_trainX[k],weights,biases,number_hidden_layers, activation_function, output_function)
      y_predicted.append(h[-1])

      if loss_function == 'cross_entropy':
        tloss += cross_entropy(h[-1],mini_batch_trainy[k])
      elif loss_function == 'mean_squared_error':
        tloss += mean_squared_error(h[-1],mini_batch_trainy[k])
      else:
        print('wrong loss function')
      
      del_W,del_b = backward_propagation(mini_batch_trainy[k],mini_batch_trainX[k],h[-1],a,h,weights,number_hidden_layers ,activation_function)

      i = 0
      while(i!=len(weights)):
        v_weights[i] = (v_weights[i]*beta_2) + (((del_W['W'+str(i+1)])**2)*(1-beta_2))
        v_biases[i] = (v_biases[i]*beta_2) + (((del_b['b'+str(i+1)])**2)*(1-beta_2))

        m_weights[i] = (m_weights[i]*beta_1) + (del_W['W' + str(i+1)]*(1-beta_1))
        m_biases[i] = (m_biases[i]*beta_1) + (del_b['b' + str(i+1)]*(1-beta_1))

        v_hat_weights[i] = (v_weights[i]/(1-beta_2**c))
        v_hat_biases[i] = (v_biases[i]/(1-beta_2**c))

        m_hat_weights[i] = (m_weights[i]/(1-beta_1**c))
        m_hat_biases[i] = (m_biases[i]/(1-beta_1**c))

        weights[i] = weights[i] - ((m_hat_weights[i]*eta/np.sqrt(v_hat_weights[i] + epsilon_)))
        biases[i] = biases[i] - ((m_hat_biases[i]*eta/np.sqrt(v_hat_biases[i] + epsilon_)))
        i+=1
      k+=1

    train_acc = train_accuracy(mini_batch_trainy,y_predicted,trainy)
    reg_term_train = calculate_regularizing_term(trainy,weight_decay_const,number_hidden_layers,weights)
    tr_loss = tloss/number_batches + reg_term_train
    val_acc = test_accuracy(valX,valy,weights,biases,number_hidden_layers,activation_function,output_function)
    vloss = val_loss(valX,valiy,weights,biases,number_hidden_layers,activation_function,output_function,loss_function)
    reg_term_val = calculate_regularizing_term(valiy,weight_decay_const,number_hidden_layers,weights)
    vloss = vloss + reg_term_val
    print("epoch : ",j+1," validation loss : ",vloss)

    train_loss_list.append(tr_loss)
    val_loss_list.append(vloss)
    train_acc_list.append(train_acc)
    val_acc_list.append(val_acc)

    if wandb_flag == True:
      wandb.log({"train_loss":tr_loss,"validation_loss":vloss,"train_accuracy":train_acc,"validation_accuracy":val_acc})

    j+=1

  plot_lists = [train_loss_list,val_loss_list,train_acc_list,val_acc_list]
  return h[-1],weights,biases,plot_lists

def nadam(trainX, trainy, number_hidden_layers = 1, hidden_layer_size = 4, eta = 0.1, initial_weights = 'random', activation_function = 'sigmoid', epochs = 1, output_function = 'softmax', mini_batch_size=4,loss_function = 'cross_entropy',weight_decay_const = 0,wandb_flag=False):
  
#initialize layers of neural networks
  layers = []
  layer1 = {'input_size' : input_layer_size, 'output_size' : hidden_layer_size, 'function' : activation_function}
  layers.append(layer1)
  
  i=0
  while(i!=number_hidden_layers-1):
    hlayer = {'input_size' : hidden_layer_size, 'output_size' : hidden_layer_size, 'function' : activation_function}
    layers.append(hlayer)
    i+=1
  
  layern = {'input_size' : hidden_layer_size, 'output_size' : output_layer_size, 'function' : output_function}
  layers.append(layern)

#initialize weights and biases

  weights,biases =initialize_weights_and_biases(layers,number_hidden_layers,initial_weights)

  number_batches = len(trainX)/mini_batch_size

  mini_batch_trainX = np.array(np.array_split(trainX, number_batches))
  mini_batch_trainy = np.array(np.array_split(trainy, number_batches))

  # beta1 = 0.9
  # beta2 = 0.999
  # ep = 1e-3

  v_weights,v_biases,v_hat_weights,v_hat_biases,m_weights,m_biases,m_hat_weights,m_hat_biases = [],[],[],[],[],[],[],[]

  i = 0
  while(i!=number_hidden_layers+1):
    v_weights.append(np.zeros((len(weights[i]),len(weights[i][0]))))
    v_biases.append(np.zeros((len(biases[i]),len(biases[i][0]))))
    v_hat_weights.append(np.zeros((len(weights[i]),len(weights[i][0]))))
    v_hat_biases.append(np.zeros((len(biases[i]),len(biases[i][0]))))
    m_weights.append(np.zeros((len(weights[i]),len(weights[i][0]))))
    m_biases.append(np.zeros((len(biases[i]),len(biases[i][0]))))
    m_hat_weights.append(np.zeros((len(weights[i]),len(weights[i][0]))))
    m_hat_biases.append(np.zeros((len(biases[i]),len(biases[i][0]))))
    i+=1

  train_loss_list = []
  val_loss_list = []
  train_acc_list = []
  val_acc_list = []
  h=None
  j = 0
  c = 0
  while(j!=epochs):
    k=0
    tloss = 0
    vloss = 0
    y_predicted = []
    while(k!=len(mini_batch_trainX)):
      lookahead_weights,lookahead_biases,lookahead_v_hat_weights,lookahead_v_hat_biases,lookahead_m_hat_weights,lookahead_m_hat_biases = [],[],[],[],[],[]
      l = 0
      c+=1
      while(l!=number_hidden_layers+1):
        lookahead_v_hat_weights.append((v_weights[l]*beta_2)/(1 - (beta_2**c)))
        lookahead_v_hat_biases.append((v_biases[l]*beta_2)/(1 - (beta_2**c)))

        lookahead_m_hat_weights.append((m_weights[l]*beta_1)/(1 - beta_1**c))
        lookahead_m_hat_biases.append((m_biases[l]*beta_1)/ (1 - beta_1**c))

        lookahead_weights.append(weights[l] - (lookahead_m_hat_weights[l] / np.sqrt(lookahead_v_hat_weights[l] + epsilon_))*eta)
        lookahead_biases.append(biases[l] - (lookahead_m_hat_biases[l] / np.sqrt(lookahead_v_hat_biases[l] + epsilon_))*eta)
        l+=1

      a,h = forward_propagation(mini_batch_trainX[k],lookahead_weights,lookahead_biases,number_hidden_layers, activation_function, output_function)
      y_predicted.append(h[-1])

      if loss_function == 'cross_entropy':
        tloss += cross_entropy(h[-1],mini_batch_trainy[k])
      elif loss_function == 'mean_squared_error':
        tloss += mean_squared_error(h[-1],mini_batch_trainy[k])
      else:
        print('wrong loss function')

      del_W,del_b = backward_propagation(mini_batch_trainy[k],mini_batch_trainX[k],h[-1],a,h,lookahead_weights,number_hidden_layers ,activation_function)

      i = 0
      while(i!=len(weights)):
        v_weights[i] = (v_weights[i]*beta_2) + (((del_W['W'+str(i+1)])**2)*(1-beta_2))
        v_biases[i] = (v_biases[i]*beta_2) + (((del_b['b'+str(i+1)])**2)*(1-beta_2))

        m_weights[i] = (m_weights[i]*beta_1) + (del_W['W' + str(i+1)]*(1-beta_1))
        m_biases[i] = (m_biases[i]*beta_1) + (del_b['b' + str(i+1)]*(1-beta_1))

        v_hat_weights[i] = (v_weights[i]/(1-beta_2**c))
        v_hat_biases[i] = (v_biases[i]/(1-beta_2**c))

        m_hat_weights[i] = (m_weights[i]/(1-beta_1**c))
        m_hat_biases[i] = (m_biases[i]/(1-beta_1**c))

        weights[i] = weights[i] - ((m_hat_weights[i]*eta/np.sqrt(v_hat_weights[i] + epsilon_)))
        biases[i] = biases[i] - ((m_hat_biases[i]*eta/np.sqrt(v_hat_biases[i] + epsilon_)))
        i+=1
      k+=1
    
    train_acc = train_accuracy(mini_batch_trainy,y_predicted,trainy)
    reg_term_train = calculate_regularizing_term(trainy,weight_decay_const,number_hidden_layers,weights)
    tr_loss = tloss/number_batches + reg_term_train
    val_acc = test_accuracy(valX,valy,weights,biases,number_hidden_layers,activation_function,output_function)
    vloss = val_loss(valX,valiy,weights,biases,number_hidden_layers,activation_function,output_function,loss_function)
    reg_term_val = calculate_regularizing_term(valiy,weight_decay_const,number_hidden_layers,weights)
    vloss = vloss + reg_term_val
    print("epoch : ",j+1," validation loss : ",vloss)

    train_loss_list.append(tr_loss)
    val_loss_list.append(vloss)
    train_acc_list.append(train_acc)
    val_acc_list.append(val_acc)

    if wandb_flag == True:
      wandb.log({"train_loss":tr_loss,"validation_loss":vloss,"train_accuracy":train_acc,"validation_accuracy":val_acc})

    j+=1

  plot_lists = [train_loss_list,val_loss_list,train_acc_list,val_acc_list]
  return h[-1],weights,biases,plot_lists

def train(trainX,trainy,textX,testy,number_hidden_layers,hidden_layer_size,eta,init_type,activation_function,epochs,mini_batch_size,loss_function,optimizer,output_function,weight_decay_const,wandb_flag=False):

  if optimizer=='sgd':
    
    hL,weights,biases,plot_list = gradient_descent(trainX,trainy,number_hidden_layers,hidden_layer_size,eta,init_type,activation_function,epochs,output_function,mini_batch_size,loss_function,weight_decay_const,wandb_flag)
    params = [weights,biases,number_hidden_layers,activation_function,output_function]
    return params
    
  elif optimizer=='momentum':
  
    hL,weights,biases,plot_list = momentum_based_gradient_descent(trainX,trainy,number_hidden_layers,hidden_layer_size,eta,init_type,activation_function,epochs,output_function,mini_batch_size,loss_function,weight_decay_const,wandb_flag)
    params = [weights,biases,number_hidden_layers,activation_function,output_function]
    return params
   
  elif optimizer=='nag':
  
    hL,weights,biases,plot_list = nestrov_accelerated_gradient_descent(trainX,trainy,number_hidden_layers,hidden_layer_size,eta,init_type,activation_function,epochs,output_function,mini_batch_size,loss_function,weight_decay_const,wandb_flag)
    params = [weights,biases,number_hidden_layers,activation_function,output_function]
    return params

  elif optimizer=='rmsprop':

    hL,weights,biases,plot_list = rmsprop(trainX,trainy,number_hidden_layers,hidden_layer_size,eta,init_type,activation_function,epochs,output_function,mini_batch_size,loss_function,weight_decay_const,wandb_flag)
    params = [weights,biases,number_hidden_layers,activation_function,output_function]
    return params

  elif optimizer == 'adam':

    hL,weights,biases,plot_list = adam(trainX,trainy,number_hidden_layers,hidden_layer_size,eta,init_type,activation_function,epochs,output_function,mini_batch_size,loss_function,weight_decay_const,wandb_flag)
    params = [weights,biases,number_hidden_layers,activation_function,output_function]

    return params

  elif optimizer == 'nadam':
    hL,weights,biases,plot_list = nadam(trainX,trainy,number_hidden_layers,hidden_layer_size,eta,init_type,activation_function,epochs,output_function,mini_batch_size,loss_function,weight_decay_const,wandb_flag)
    params = [weights,biases,number_hidden_layers,activation_function,output_function]
    return params

def train_data(config = None):
  
  with wandb.init(config = config,entity=entity_name) as run:
    config = wandb.config
    run.name = 'hl_'+str(config.nhl)+'_bs_'+str(config.bs)+'_ac_'+config.af
    print(config)
    train(trainX,trainy,testX,needed_y_test,config.nhl,config.hls,config.eta,config.wi,config.af,config.epochs,config.bs,'cross_entropy',config.opt,'softmax',config.wdc,True)

# wandb.agent(sweep_id,train_data,count=60)

# v = [proj_name,entity_name,data_set,ep,bs,lf,op,lr,m_beta,rmsprop_beta,beta_1,beta_2,epsilon_,wdc,winit,nhls,hls,af]
# print(v)
params = train(trainX,trainy,testX,needed_y_test,nhls,hls,lr,winit,af,ep,bs,lf,op,'softmax',wdc,False)
test_ac = test_accuracy(testX,needed_y_test,params[0],params[1],params[2],params[3],params[4])
print("Test accuracy on best model = ", test_ac*100,'%')

# run = wandb.init(project="A1_DL", entity="cs22m064")
# run.name = 'Confusion Matrix'

a,h = forward_propagation(testX,params[0],params[1],params[2], params[3], params[4])
hL = h[-1]
y_pred = np.zeros(len(hL))
i = 0
while(i!=len(hL)):
  y_pred[i] = np.argmax(hL[i])
  i+=1
y_pred

# wandb.log({"The Confusion Marix": wandb.plot.confusion_matrix(preds = y_pred,y_true=test_Y,class_names=classes)})
# wandb.save()
# wandb.finish()
